feat(cli): initial public release of yak-ctx-client üêº ‚Äî context-engineered OpenAI-compatible REPL

## Motivation
This CLI client implements practical ‚Äúeffective context engineering‚Äù patterns inspired by
Anthropic‚Äôs *Effective Context Engineering for AI Agents* (2025). It aims to make small,
local models more reliable over long conversations by structuring prompts and managing
context explicitly rather than hoping the model ‚Äújust remembers.‚Äù

## Purpose
Provide an ergonomic, batteries-included REPL for OpenAI-compatible /v1 endpoints
(e.g., vLLM) that:
- Preserves conversation history on disk and exports transcripts
- Compacts older turns into a durable, model-visible summary
- Surfaces ‚Äúrelevant memory‚Äù (simple KV facts) inline
- Keeps a single, sectioned system message so servers/templates don‚Äôt drop later system prompts

## Key Features
- Single, sectioned **system message** with:
  - üêº Role
  - üìè Ground Rules
  - üß† Relevant Memory
  - üßæ Conversation Summary
  - ‚úçÔ∏è Output Style

- **Adaptive compaction**: summarize older turns into a concise brief that‚Äôs re-injected into
  the system block (not as a low-weight assistant message).
- **Tokenizer-aware budgeting**: prefers HF tokenizer for Qwen (falls back to tiktoken, then heuristic)
  to reduce ‚Äúphantom trims.‚Äù
- **Memory KV store** with naive relevance selection.
- **REPL UX** with emoji commands:
  `/help, /mem, /reset, /export, /stats, /history, /tail, /setsys, /loadsys, /saveas, /debug`
- **Transcripts** written as JSONL in `./yak_client_data/transcripts/`.
- **Config via env**: `OPENAI_API_BASE`, `OPENAI_API_KEY`, `OPENAI_MODEL`, token budgets, etc.
- **Debug mode** prints the exact system block + first N messages sent to the server.

## Design Choices
- **Single system message**: avoids server/template quirks where additional system entries
  are ignored or down-weighted.
- **Summary lives in system**: makes the synopsis ‚Äúauthoritative‚Äù for the model.
- **Keep tail pairs** adaptively (3‚Äì6 pairs by default), then summarize the head.
- **Explicit logging** and `/stats` for visibility into what the model actually sees.

## Getting Started

1) Configure your endpoint (example):
- export OPENAI_API_BASE="http://yak:8000/v1"
- export OPENAI_API_KEY="sk-local-123"
- export OPENAI_MODEL="Qwen/Qwen2.5-7B-Instruct"

2) Run:

- python yc.py "What's the good word for the day?" # one time query

or interactive:

- python yc.py

3) Explore `/help` for commands.

## References
- Anthropic, ‚ÄúEffective Context Engineering for AI Agents‚Äù (2025)

## Notes
- Works well on Qwen2.5-7B-Instruct; larger variants (14B/32B) improve long-horizon fidelity.
- The client is endpoint-agnostic as long as it speaks the OpenAI `/chat/completions` API.


## Install
WSL/Linux:
- git clone https://github.com/jrtorrez31337/yakchat.git
- cd yakchat
- python -m venv yakchat 
- source .yakchat/bin/activate  
- pip install -r requirements.txt
